 Jared Turkewitz writesâ€”

Here is my code: https://github.com/jturkewitz/SideProjects/tree/master/Kaggle/Telstra This was the 4th place solution. In the end I used a weighted ensemble of 5 xgboost models and 2 extra tree classifiers. For feature engineering I did: Used the magic feature of row number in severity ordered by location (and its reverse order) Grouping locations by their properties (like number of ids, median event, most common log feature etc) Taking aggregate of all log feature / resource type combinations and hashing after sorting by median log feature etc - also group these by number of ids with the same log features Log feature dummies multiplied by their volume (so if a column had log_feature_203 with volume 7 there would be a log_feature_203 column with a value of 7) Same as above but first bin log_feature into bins of 10 log_feature numbers Counts of log_feature, event_type etc Statistical quantities like the mean, median, std, min, max for resource type, event type and log_feature Grouping log_feature by their most common location And some other feature engineering you can see at the code if you are interested.

I tried stacking, but could not get it to work so I just went with using a weighted average of the models. The extra tree classifier provided a good boost, somewhat unexpectedly. I used it after seeing a high-performing script with it in the BNP competition. I could not get neural nets or random forests to help my ensemble.
